services:
  change-vol-ownership:
    image: alpine:3.22.1
    user: "root"
    group_add:
      - 2048
    volumes:
      - app_volume:/tmp/change-app-ownership
      - static_volume:/tmp/change-static-ownership
      - media_volume:/tmp/change-media-ownership
      - logs_volume:/tmp/change-logs-ownership
    command: chown -R 2048:2048 /tmp/change-app-ownership /tmp/change-static-ownership /tmp/change-media-ownership /tmp/change-logs-ownership

  postgres:
    image: postgres
    env_file:
      - .env
    ports:
      - "5432:5432"
    healthcheck:
      test:
        - CMD-SHELL
        - "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB} -p ${POSTGRES_PORT}"
      interval: 5s
      timeout: 5s
      retries: 5
    volumes:
      - "prod_db:/var/lib/postgresql/data"
    profiles:
      - with-db

  app:
    build:
      context: .
      dockerfile: base-prod.dockerfile
    ports:
      - "8000:8000"
    depends_on:
      change-vol-ownership:
        condition: service_completed_successfully
    env_file:
      - .env
    environment:
      ENV: prod
    healthcheck:
      test: ["CMD-SHELL", "netstat -ltn | grep -q 8000"]
      timeout: 5s
      retries: 3
    volumes:
      - static_volume:/zango/zango_project/static
      - media_volume:/zango/zango_project/media
      - app_volume:/zango/zango_project/workspaces
      - logs_volume:/zango/zango_project/logs

  nginx:
    build:
      context: .
      dockerfile: config/nginx.dockerfile
    ports:
      - "1443:1443"
    env_file:
      - .env
    volumes:
      - static_volume:/zango/zango_project/static:ro
      - media_volume:/zango/zango_project/media:ro
    depends_on:
      app:
        condition: service_healthy
      change-vol-ownership:
        condition: service_completed_successfully

  celery:
    build:
      context: .
      dockerfile: base-prod.dockerfile
    entrypoint: /bin/sh -c "cd zango_project && celery -A zango_project worker -l info"
    env_file:
      - .env
    volumes:
      - app_volume:/zango/zango_project/workspaces
      - logs_volume:/zango/zango_project/logs
    depends_on:
      app:
        condition: service_healthy
      redis:
        condition: service_healthy
      change-vol-ownership:
        condition: service_completed_successfully

  redis:
    image: redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  celery-flower:
    build:
      context: .
      dockerfile: base-prod.dockerfile
    restart: always
    entrypoint:
      - /bin/sh 
      - start_flower.sh
    ports:
      - "5555:5555"
    env_file:
      - .env
    environment:
      - FLOWER_PORT=5555
      - FLOWER_PERSISTENT=True
      - FLOWER_STATE_SAVE_INTERVAL=10000
    volumes:
      - app_volume:/zango/zango_project/workspaces
      - logs_volume:/zango/zango_project/logs
    depends_on:
      app:
        condition: service_healthy
      redis:
        condition: service_healthy
      celery:
        condition: service_started
      change-vol-ownership:
        condition: service_completed_successfully

  celery_beat:
    build:
      context: .
      dockerfile: base-prod.dockerfile
    entrypoint: /bin/sh -c "cd zango_project && single-beat celery -A zango_project beat -l info --scheduler django_celery_beat.schedulers:DatabaseScheduler"
    env_file:
      - .env
    volumes:
      - app_volume:/zango/zango_project/workspaces
      - logs_volume:/zango/zango_project/logs
    depends_on:
      app:
        condition: service_healthy
      redis:
        condition: service_healthy
      celery:
        condition: service_started
      change-vol-ownership:
        condition: service_completed_successfully

  otel-collector:
    build:
      context: .
      dockerfile: config/otel-collector.dockerfile
    entrypoint: ["/start_otel.sh"]
    env_file:
      - .env
    ports:
      - 4317:4317
      - 4318:4318
      - 9090:9090
    profiles:
      - with-instrumentation
    
volumes:
  prod_db: null
  app_volume: null
  static_volume: null
  media_volume: null
  logs_volume: null
